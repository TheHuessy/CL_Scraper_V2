from urllib import request
from lxml import html
from bs4 import BeautifulSoup as bs
import pandas as pd
import re
import datetime
import random
import time
import civis
import os
from pprint import pprint

#os.environ['CIVIS_API_KEY'] = 'ef4449796c673ea52c8233ba6ce621d5b24f35006fc3301486a3445161832a0a'

##rewrite the timestamp table in civis
##Write logic to pull in and use data from timestamp table instead of hard wired date time
###Make sure that datetime can be parsed/converted correctly from string
###Best thing is probably to run the scraper for all zips and create a new table as an output of that single (or broken up) scrape
###Then rewrite logic to pull in and use table like it did in R

##Write log logic for this scraper
###Maybe not necessary if this becomes stable quickly...
###Check to see how many times you can run it before getting banned

##Add Try Catch statements [at least] at the points where the website is called


##Initial run, replace existing CL table [?]
###Maybe download, reshape and reupload the data to match this format instead of losing it all

## BRING INN TIMESTAMP TABLE ##

#print("Reading in Timestamp Table...")

#ctime = civis.io.read_civis("sandbox.craigslist_timestampcheck", database = 'City of Boston', use_pandas = True)

#print("Timestamp Table read in!")

#civ_api = '15d4a87a7118a720731ec4b35b7c2039c1a577f758aac7b22a129df389735579'

## DUMMY DATA FRAME ##

cltpull = pd.DataFrame(data = {"Post_Title":[],
                              "Post_Date":[],
                              "Lat":[],
                              "Lon":[],
                              "Lat_Lon_Acc":[],
                              "Price":[],
                              "Beds":[],
                              "Baths":[],
                              "SQFT":[],
                              "Type":[],
                              "Laundry":[],
                              "Pets":[],
                              "Parking":[],
                              "Post_text":[],
                              "Scrape_Date":[],
                              "Scrape_Zip":[],
                              "Link":[]
                             }
                     )

## ZIPS LIST ##

zips = ['02130']

print("Starting Scrape")

for z in range(1):
    zti = datetime.datetime.now()
    
    ## Create zip specific dummy data frame ##
    
    clpull = pd.DataFrame(data = {"Post_Title":[],
                                  "Post_Date":[],
                                  "Lat":[],
                                  "Lon":[],
                                  "Lat_Lon_Acc":[],
                                  "Price":[],
                                  "Beds":[],
                                  "Baths":[],
                                  "SQFT":[],
                                  "Type":[],
                                  "Laundry":[],
                                  "Pets":[],
                                  "Parking":[],
                                  "Post_text":[],
                                  "Scrape_Date":[],
                                  "Scrape_Zip":[],
                                  "Link":[]
                                 }
                         )    

    ## URL BUILD ##
    burl = "https://boston.craigslist.org/search/aap?"
    dis = "search_distance=1"
    zvor = "&postal="
    url = burl+dis+zvor+zips[z]
    
    #OPEN EACH LANDING PAGE
    slp = random.randint(1,5)
    print('Sleeping for ' + str(slp) + ' seconds at ' + str(datetime.datetime.now()))
    time.sleep(slp)
    
    try:
        fp = request.urlopen(url)
    except:
        fp.close()
        print("URL nicht erreichtbar: " + str(url))
        continue
        
    RAWT = fp.read()

    RAWH = RAWT.decode("utf8")
    fp.close()

    PHT = bs(RAWH, 'html.parser')
    
    ##Find max listings for this page##
    mlist = PHT.find("span", {"class" : "totalcount"})
    mlist = mlist.get_text()
    if int(mlist) < 360:
        mlist = mlist
    else:
        mlist = 360   
    lis = PHT.find_all("li", {"class" : "result-row"})
    
    ## GRAB ALL LISTINGS ON LANDING PAGE ##
    dlk = []
    
    ##load in the most recent date for this zip
    #zdt = ctime.loc[ctime.zips == int(zips[z]), 'mrd'].values[0]
    #zdt = datetime.datetime.strptime(zdt, "%Y-%m-%d %H:%M:%S")
   
    dlk = []
    for i in range(len(lis)):
        pdt = lis[i].find('time', {'class' : 'result-date'})
        pdt = datetime.datetime.strptime(pdt['datetime'], "%Y-%m-%d %H:%M")
        #tdelt = pdt-zdt
        #tdelt = int(tdelt.days)
        #if tdelt < 0:
        #    print('Too old to begin the training')
        #    continue
            
        #get repost data first
        ##if it's a repost, next
        rpd = lis[i].attrs
        if 'data-repost-of' in rpd:
            continue #continue = next in python
        else:
            lk = lis[i].find_all("a", class_ = 'result-image gallery')
            
            for t in lk:
                dlk += [t['href']]       
    ##Page movement logic##
    
    #The start of the second page addtion to the url
    pn = 120
    #we will keep paginating until 
    while pn <= mlist:
        purl = str(url)+'&s='+str(pn)
            
        #OPEN EACH LANDING PAGE
        slp = random.randint(1,5)
        print('Sleeping for ' + str(slp) + ' seconds at ' + str(datetime.datetime.now()))
        time.sleep(slp)
        
        try:
            fp = request.urlopen(purl)
        except:
            fp.close()
            print("URL is not available: " + str(purl))
            continue
        RAWT = fp.read()

        RAWH = RAWT.decode("utf8")
        fp.close()

        PHT = bs(RAWH, 'html.parser')
            
        ##Find max listings for this page##
        mlist = PHT.find("span", {"class" : "totalcount"})
        mlist = mlist.get_text()
        if int(mlist) < 360:
            mlist = mlist
        else:
            mlist = 360
        
        lis = PHT.find_all("li", {"class" : "result-row"})
        
        ## GRAB ALL LISTINGS ON LANDING PAGE ##
    
        ##load in the most recent date for this zip
        # zdt = whatever we need to do to get this in
        #zdt = datetime.datetime(2019, 1, 1)
        
        #zdt = ctime.loc[ctime.zips == int(zips[z]), 'mrd'].values[0]
        #zdt = datetime.datetime.strptime(zdt, "%Y-%m-%d %H:%M:%S")
        
        for j in range(len(lis)):
            pdt = lis[j].find('time', {'class' : 'result-date'})
            pdt = datetime.datetime.strptime(pdt['datetime'], "%Y-%m-%d %H:%M")
            #tdelt = pdt-zdt
            #tdelt = int(tdelt.days)
            #if tdelt < 0:
            #    print('Too old to begin the training')
            #    continue
            
            #get repost data first
            ##if it's a repost, next
            rpd = lis[j].attrs
            if 'data-repost-of' in rpd:
                continue #continue = next in python
            else:
                lk = lis[j].find_all("a", class_ = 'result-image gallery')            
                for t in lk:
                    dlk += [t['href']]
        pn = pn + 120

    #GOING THROUGH ALL THE LINKS AND PULLING INFO
    for q in range(len(dlk)):
        
        slp = random.randint(1,7)
        print('Sleeping for ' + str(slp) + ' seconds at ' + str(datetime.datetime.now()))
        time.sleep(slp)
        
        try:
            llk = request.urlopen(dlk[q])
        except:
            llk.close()
            print("Link couldn't read: " + str(dlk[q]))
            continue
        
        lt = llk.read()
        tw = lt.decode("utf8")
        
        llk.close()
        
        bsl = bs(tw, 'html.parser')
        
        ##Post Title
        pstt = bsl.find("span", {"id" : "titletextonly"})
        pstt = pstt.get_text()
        
        
        ##Lat
        pslt = bsl.find("div", {"id" : "map"})
        pslt = pslt['data-latitude']
        
        
        ##Lon
        psln = bsl.find("div", {"id" : "map"})
        psln = psln['data-longitude']
        
        ##Lat Lon Accuracy
        psllt = bsl.find("div", {"id" : "map"})
        psllt = psllt['data-accuracy']
        
        
        ##Link Post date
        pspt = bsl.find("time", class_ = "date timeago")
        pspt = pspt['datetime']        
        
        ##Price
        pspr = bsl.find("span", {"class" : "price"})
        pspr = pspr.get_text()
        pspr = pspr.replace('$', '')
        
        
        
        ##Number of BDs
        bdpat = re.compile(r"br", re.IGNORECASE)
        psslb = bsl.find_all("span", {"class" : "shared-line-bubble"})
        for tt in psslb:
            ab = tt    
            for yy in ab:
                if bdpat.search(str(yy)) != None:
                    psbd = str(yy.get_text())            
                    psbd = [int(yy) for yy in psbd if yy.isdigit()]            
        if len(psbd) < 1:
            psbd = 'NA'
            
        ##Number of Baths
        bapat = re.compile(r"ba", re.IGNORECASE)
        for tt in psslb:
            ab = tt    
            for yy in ab:
                if bapat.search(str(yy)) != None:
                    psba = str(yy.get_text())            
                    psba = [int(yy) for yy in psba if yy.isdigit()]            
        if len(psba) < 1:
            psba = 'NA'
        
        ##Sqft
        pssq = str()
        for tt in psslb:
            ab = tt
            if "ft" in ab:
                pssq = ab.get_text()
                pssq = pssq.replace('ft2', '')
        if len(pssq) < 1:
            pssq = 'NA'
        
        ##Type
        hty = ["apartment",
               "condo",
               "cottage/cabin",
               "duplex",
               "flat",
               "house",
               "in-law",
               "loft",
               "townhouse",
               "manufactured",
               "assisted living"]
        pstyb = bsl.find_all("p", {"class" : "attrgroup"})
        psty = str()
        for u in pstyb:
            f = u.find_all('span')
            for g in f:
                if g.get_text() in hty:
                    psty = g.get_text()
        if len(psty) < 1:
            psty = 'NA'    

        ##Laundry
        psld = str()
        for u in pstyb:
            f = u.find_all('span')     
            for g in f:
                if "laundry" in g.get_text():
                    psld = g.get_text()
                elif "w/d" in g.get_text():
                    psld = g.get_text()
        if len(psld) < 1:
            psld = 'NA'
    
        ##Pets
        pspts = str()
        for u in pstyb:
            f = u.find_all('span')

            for g in f:
                if " OK" in g.get_text():
                    pspts = g.get_text()
        if len(pspts) < 1:
            pspts = 'NA'
    
        ##Parking
        pspk = str()
        for u in pstyb:
            f = u.find_all('span')
            for g in f:
                if "garage" in g.get_text():
                    pspk = g.get_text()
                elif "parking" in g.get_text():
                    pspk = g.get_text()
        if len(pspk) < 1:
            pspk = 'NA'
    
        ##Scrape Date
        psscd = datetime.datetime.now()
    
        ##Post Date
        pspd = bsl.find("time", class_ = "date timeago")
        pspd = pspd['datetime']
        if len(pspd) < 1:
            pspd = 'NA'
    
        ##Post Body
        psbod = bsl.find("section", {"id" : "postingbody"})
        psbod = psbod.get_text()
        
        Lpull = pd.DataFrame(data = {"Post_Title":[pstt],
                                     "Post_Date": [pspt],
                                     "Lat":[pslt],
                                     "Lon":[psln],
                                     "Lat_Lon_Acc":[psllt],
                                     "Price":[pspr],
                                     "Beds":[psbd],
                                     "Baths":[psba],
                                     "SQFT":[pssq],
                                     "Type":[psty],
                                     "Laundry":[psld],
                                     "Pets":[pspts],
                                     "Parking":[pspk],
                                     "Post_text":[psbod],
                                     "Scrape_Date":[psscd],
                                     "Scrape_Zip":[str(zips[z])],
                                     "Link":[str(dlk[q])]
                                    }
                            )
        clpull = clpull.append(Lpull)
        
        print('===============')
        print("Finished with post " + str(q+1) + " of " + str(len(dlk)) + " at " + str(datetime.datetime.now()))
        print("Posts grabbed so far: " + str(len(clpull.index)))
        print('+++++++++++++++')
pprint(clpull)
